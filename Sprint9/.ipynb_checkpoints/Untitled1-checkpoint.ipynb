{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価指標\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train_one_hot = (y_train.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "y_test_one_hot = (y_test.reshape(-1, 1) == np.arange(10)).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, permutation, val_size_rate=0.2):\n",
    "    data = data[permutation]\n",
    "    val_size = int(len(data) * val_size_rate)\n",
    "    val = data[:val_size]\n",
    "    train = data[val_size:]\n",
    "    return train, val\n",
    "\n",
    "permutation = np.random.permutation(np.arange(len(x_train)))\n",
    "x_train, x_val = split_data(x_train, permutation)\n",
    "y_train, y_val = split_data(y_train, permutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スーパーエンジニアの方の模写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"Calculates log(sum(exp(x))).\n",
    "    \"\"\"\n",
    "    xmax = x.max(axis=1, keepdims=True)\n",
    "    return np.log(np.exp(x - xmax).sum(axis=1, keepdims=True)) + xmax\n",
    "\n",
    "\n",
    "class ScratchSimpleNeuralNetrowkClassifier:\n",
    "    \"\"\"MNISTを学習する3層ニューラルネットワーク\n",
    "\n",
    "    Parameter:\n",
    "    ----------\n",
    "    seed (int): シード値\n",
    "    verbose (bool): ログ出力するかどうかのflag\n",
    "    \"\"\"\n",
    "\n",
    "    n1, n2, n3, n4 = 784, 400, 200, 10\n",
    "\n",
    "    def __init__(self, seed=0, verbose = True):\n",
    "         # Problem 1. (initialization)\n",
    "        self.verbose = verbose\n",
    "        sigma = 0.01\n",
    "        np.random.seed(seed)\n",
    "        self.w1 = np.random.normal(0, sigma, (self.n1, self.n2))  # self.w1.shape == (784, 400)\n",
    "        self.b1 = np.random.normal(0, sigma, self.n2)  # self.b1.shape == (400,)\n",
    "        self.w2 = np.random.normal(0, sigma, (self.n2, self.n3))  # self.w2.shape == (400, 200)\n",
    "        self.b2 = np.random.normal(0, sigma, self.n3)  # self.b2.shape == (200,)\n",
    "        self.w3 = np.random.normal(0, sigma, (self.n3, self.n4))  # self.w3.shape == (200, 10)\n",
    "        self.b3 = np.random.normal(0, sigma, self.n4)  # self.b3.shape == (10,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward propergation\n",
    "\n",
    "        Parameter:\n",
    "        ----------\n",
    "        x (ndarra): 平滑化された画像データ (batch_size, 784) float64\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        log_z3: 各画像データとラベルの組についての確率の対数 (batch_size, 10) float64\n",
    "        \"\"\"\n",
    "        # Problem 2. (forward propagation)\n",
    "        batch_size = x.shape[0]\n",
    "        self.x = x.reshape(batch_size, 784)  # self.x.shape == (batch_size, 784)\n",
    "        self.a1 = self.x.dot(self.w1) + self.b1  # self.a1.shape == (batch_size, 400)\n",
    "        self.z1 = np.tanh(self.a1)  # self.z1.shape == (batch_size, 400)\n",
    "        self.a2 = self.z1.dot(self.w2) + self.b2  # self.a2.shape == (batch_size, 200)\n",
    "        self.z2 = np.tanh(self.a2)  # self.z2.shape == (batch_size, 200)\n",
    "        self.a3 = self.z2.dot(self.w3) + self.b3  # self.a3.shape == (batch_size, 10)\n",
    "        # 以下の式は a3 の値が大きい時に overflow するので避ける\n",
    "        # self.z3 = np.exp(self.a3) / np.exp(self.a3).sum(axis=1, keepdims=True)\n",
    "        log_z3 = self.a3 - logsumexp(self.a3)\n",
    "        self.z3 = np.exp(log_z3)\n",
    "        return log_z3\n",
    "\n",
    "    def forward_and_loss(self, x, y):\n",
    "        \"\"\"forward propergation + loss\n",
    "\n",
    "        Parameter:\n",
    "        ----------\n",
    "        x (ndarray): 平滑化された画像データ (batch_size, 784) float64\n",
    "        y (ndarray): 正解ラベル (batch_size,) int64\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss (float64): loss値\n",
    "        correct (int64): 正答数\n",
    "        \"\"\"\n",
    "        # Problem 3. (cross entropy)\n",
    "        self.y = y.reshape(-1, 1) == np.arange(10)  # one-hot 表現に変換する\n",
    "        log_z3 = self.forward(x)\n",
    "        loss = -(self.y * log_z3).mean(axis=0).sum()\n",
    "        correct = (self.z3.argmax(axis=1) == y).sum()\n",
    "        return loss, correct\n",
    "        \n",
    "    def backprop(self, alpha=0.01):\n",
    "        # Problem 4. (back propagation)\n",
    "        batch_size = self.x.shape[0]\n",
    "        ga3 = (self.z3 - self.y) / batch_size\n",
    "        self.b3 -= alpha * ga3.sum(axis=0)\n",
    "        self.w3 -= alpha * self.z2.T.dot(ga3)\n",
    "        gz2 = ga3.dot(self.w3.T)\n",
    "        ga2 = gz2 * (1 - self.z2 ** 2)\n",
    "        self.b2 -= alpha * ga2.sum(axis=0)\n",
    "        self.w2 -= alpha * self.z1.T.dot(ga2)\n",
    "        gz1 = ga2.dot(self.w2.T)\n",
    "        ga1 = gz1 * (1 - self.z1 ** 2)\n",
    "        self.b1 -= alpha * ga1.sum(axis=0)\n",
    "        self.w1 -= alpha * self.x.T.dot(ga1)\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val, batch_size=20):\n",
    "\n",
    "        n_epochs = 20\n",
    "        n_step_iteration_report = 500\n",
    "        plot_data = []\n",
    "\n",
    "        # Problem 6.\n",
    "        for epoch in range(n_epochs):\n",
    "            sum_of_loss = 0\n",
    "            for i, (x, y) in enumerate(get_minibatches(x_train, y_train, batch_size)):\n",
    "                self.y = y.reshape(-1, 1) == np.arange(10)\n",
    "                loss, correct = self.forward_and_loss(x, y)\n",
    "                self.backprop()\n",
    "               \n",
    "                # ログ出力\n",
    "                sum_of_loss += loss\n",
    "                if self.verbose and (i + 1) % n_step_iteration_report == 0:\n",
    "                    train_loss = sum_of_loss / n_step_iteration_report\n",
    "                    val_loss, correct = self.forward_and_loss(x_val, y_val)\n",
    "                    print(f'epoch: {epoch+1}, iteration: {i+1}, train_loss: {train_loss:.3}, val_loss: {val_loss:.3}, accuracty: {correct / len(y_val):.3}')\n",
    "                    sum_of_loss = 0\n",
    "\n",
    "                    iters_per_epoch = len(x_train) / batch_size\n",
    "                    plot_data.append((epoch + (i + 1) / iters_per_epoch, train_loss, val_loss))\n",
    "\n",
    "        #  学習曲線のプロット\n",
    "        if self.verbose:\n",
    "            epochs, train_loss, val_loss = zip(*plot_data)\n",
    "            plt.plot(epochs, train_loss, color='r', label='train_loss')\n",
    "            plt.plot(epochs, val_loss, color='b', label='val_loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Problem 5.\n",
    "        return self.forward(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(x, y, batch_size):\n",
    "    \"\"\"Returns a generator emits mini-batches.\n",
    "    \"\"\"\n",
    "    size = x.shape[0]\n",
    "    n_batches = size // batch_size\n",
    "    shuffle_index = np.random.permutation(np.arange(size))\n",
    "    new_size = batch_size * n_batches\n",
    "    x_batches = np.split(x[shuffle_index][:new_size], n_batches)\n",
    "    y_batches = np.split(y[shuffle_index][:new_size], n_batches)\n",
    "    return zip(x_batches, y_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, iteration: 500, train_loss: 2.29, val_loss: 2.28, accuracty: 0.355\n",
      "epoch: 1, iteration: 1000, train_loss: 2.23, val_loss: 2.09, accuracty: 0.367\n",
      "epoch: 1, iteration: 1500, train_loss: 1.66, val_loss: 1.28, accuracty: 0.588\n",
      "epoch: 1, iteration: 2000, train_loss: 1.01, val_loss: 0.829, accuracty: 0.761\n",
      "epoch: 2, iteration: 500, train_loss: 0.637, val_loss: 0.587, accuracty: 0.831\n",
      "epoch: 2, iteration: 1000, train_loss: 0.528, val_loss: 0.524, accuracty: 0.849\n",
      "epoch: 2, iteration: 1500, train_loss: 0.483, val_loss: 0.48, accuracty: 0.863\n",
      "epoch: 2, iteration: 2000, train_loss: 0.451, val_loss: 0.445, accuracty: 0.872\n",
      "epoch: 3, iteration: 500, train_loss: 0.402, val_loss: 0.407, accuracty: 0.885\n",
      "epoch: 3, iteration: 1000, train_loss: 0.369, val_loss: 0.39, accuracty: 0.89\n",
      "epoch: 3, iteration: 1500, train_loss: 0.374, val_loss: 0.38, accuracty: 0.892\n",
      "epoch: 3, iteration: 2000, train_loss: 0.366, val_loss: 0.37, accuracty: 0.893\n",
      "epoch: 4, iteration: 500, train_loss: 0.339, val_loss: 0.361, accuracty: 0.898\n",
      "epoch: 4, iteration: 1000, train_loss: 0.334, val_loss: 0.349, accuracty: 0.9\n",
      "epoch: 4, iteration: 1500, train_loss: 0.334, val_loss: 0.346, accuracty: 0.899\n",
      "epoch: 4, iteration: 2000, train_loss: 0.326, val_loss: 0.334, accuracty: 0.904\n",
      "epoch: 5, iteration: 500, train_loss: 0.309, val_loss: 0.328, accuracty: 0.906\n",
      "epoch: 5, iteration: 1000, train_loss: 0.307, val_loss: 0.325, accuracty: 0.906\n",
      "epoch: 5, iteration: 1500, train_loss: 0.29, val_loss: 0.317, accuracty: 0.91\n",
      "epoch: 5, iteration: 2000, train_loss: 0.308, val_loss: 0.314, accuracty: 0.91\n",
      "epoch: 6, iteration: 500, train_loss: 0.277, val_loss: 0.306, accuracty: 0.912\n",
      "epoch: 6, iteration: 1000, train_loss: 0.285, val_loss: 0.299, accuracty: 0.913\n",
      "epoch: 6, iteration: 1500, train_loss: 0.285, val_loss: 0.293, accuracty: 0.917\n",
      "epoch: 6, iteration: 2000, train_loss: 0.272, val_loss: 0.292, accuracty: 0.918\n",
      "epoch: 7, iteration: 500, train_loss: 0.265, val_loss: 0.283, accuracty: 0.92\n",
      "epoch: 7, iteration: 1000, train_loss: 0.261, val_loss: 0.28, accuracty: 0.921\n",
      "epoch: 7, iteration: 1500, train_loss: 0.263, val_loss: 0.274, accuracty: 0.921\n",
      "epoch: 7, iteration: 2000, train_loss: 0.259, val_loss: 0.272, accuracty: 0.922\n",
      "epoch: 8, iteration: 500, train_loss: 0.237, val_loss: 0.267, accuracty: 0.924\n",
      "epoch: 8, iteration: 1000, train_loss: 0.253, val_loss: 0.268, accuracty: 0.924\n",
      "epoch: 8, iteration: 1500, train_loss: 0.244, val_loss: 0.259, accuracty: 0.926\n",
      "epoch: 8, iteration: 2000, train_loss: 0.239, val_loss: 0.255, accuracty: 0.926\n",
      "epoch: 9, iteration: 500, train_loss: 0.237, val_loss: 0.25, accuracty: 0.928\n",
      "epoch: 9, iteration: 1000, train_loss: 0.226, val_loss: 0.246, accuracty: 0.93\n",
      "epoch: 9, iteration: 1500, train_loss: 0.224, val_loss: 0.242, accuracty: 0.931\n",
      "epoch: 9, iteration: 2000, train_loss: 0.217, val_loss: 0.239, accuracty: 0.932\n",
      "epoch: 10, iteration: 500, train_loss: 0.202, val_loss: 0.235, accuracty: 0.932\n",
      "epoch: 10, iteration: 1000, train_loss: 0.215, val_loss: 0.237, accuracty: 0.933\n",
      "epoch: 10, iteration: 1500, train_loss: 0.212, val_loss: 0.23, accuracty: 0.933\n",
      "epoch: 10, iteration: 2000, train_loss: 0.198, val_loss: 0.224, accuracty: 0.936\n",
      "epoch: 11, iteration: 500, train_loss: 0.2, val_loss: 0.219, accuracty: 0.937\n",
      "epoch: 11, iteration: 1000, train_loss: 0.197, val_loss: 0.222, accuracty: 0.936\n",
      "epoch: 11, iteration: 1500, train_loss: 0.191, val_loss: 0.215, accuracty: 0.939\n",
      "epoch: 11, iteration: 2000, train_loss: 0.185, val_loss: 0.213, accuracty: 0.939\n",
      "epoch: 12, iteration: 500, train_loss: 0.183, val_loss: 0.21, accuracty: 0.939\n",
      "epoch: 12, iteration: 1000, train_loss: 0.186, val_loss: 0.207, accuracty: 0.94\n",
      "epoch: 12, iteration: 1500, train_loss: 0.18, val_loss: 0.205, accuracty: 0.94\n",
      "epoch: 12, iteration: 2000, train_loss: 0.167, val_loss: 0.201, accuracty: 0.942\n",
      "epoch: 13, iteration: 500, train_loss: 0.179, val_loss: 0.198, accuracty: 0.943\n",
      "epoch: 13, iteration: 1000, train_loss: 0.167, val_loss: 0.195, accuracty: 0.944\n",
      "epoch: 13, iteration: 1500, train_loss: 0.168, val_loss: 0.193, accuracty: 0.943\n",
      "epoch: 13, iteration: 2000, train_loss: 0.169, val_loss: 0.194, accuracty: 0.944\n",
      "epoch: 14, iteration: 500, train_loss: 0.163, val_loss: 0.189, accuracty: 0.945\n",
      "epoch: 14, iteration: 1000, train_loss: 0.162, val_loss: 0.185, accuracty: 0.946\n",
      "epoch: 14, iteration: 1500, train_loss: 0.159, val_loss: 0.183, accuracty: 0.947\n",
      "epoch: 14, iteration: 2000, train_loss: 0.154, val_loss: 0.182, accuracty: 0.947\n",
      "epoch: 15, iteration: 500, train_loss: 0.147, val_loss: 0.179, accuracty: 0.949\n",
      "epoch: 15, iteration: 1000, train_loss: 0.15, val_loss: 0.177, accuracty: 0.948\n",
      "epoch: 15, iteration: 1500, train_loss: 0.151, val_loss: 0.176, accuracty: 0.948\n",
      "epoch: 15, iteration: 2000, train_loss: 0.152, val_loss: 0.173, accuracty: 0.949\n",
      "epoch: 16, iteration: 500, train_loss: 0.143, val_loss: 0.171, accuracty: 0.949\n",
      "epoch: 16, iteration: 1000, train_loss: 0.142, val_loss: 0.166, accuracty: 0.952\n",
      "epoch: 16, iteration: 1500, train_loss: 0.146, val_loss: 0.168, accuracty: 0.951\n",
      "epoch: 16, iteration: 2000, train_loss: 0.135, val_loss: 0.168, accuracty: 0.951\n",
      "epoch: 17, iteration: 500, train_loss: 0.125, val_loss: 0.166, accuracty: 0.95\n",
      "epoch: 17, iteration: 1000, train_loss: 0.131, val_loss: 0.161, accuracty: 0.954\n",
      "epoch: 17, iteration: 1500, train_loss: 0.133, val_loss: 0.162, accuracty: 0.954\n",
      "epoch: 17, iteration: 2000, train_loss: 0.134, val_loss: 0.158, accuracty: 0.955\n",
      "epoch: 18, iteration: 500, train_loss: 0.124, val_loss: 0.155, accuracty: 0.955\n",
      "epoch: 18, iteration: 1000, train_loss: 0.125, val_loss: 0.155, accuracty: 0.955\n",
      "epoch: 18, iteration: 1500, train_loss: 0.124, val_loss: 0.155, accuracty: 0.955\n",
      "epoch: 18, iteration: 2000, train_loss: 0.118, val_loss: 0.151, accuracty: 0.956\n",
      "epoch: 19, iteration: 500, train_loss: 0.121, val_loss: 0.149, accuracty: 0.957\n",
      "epoch: 19, iteration: 1000, train_loss: 0.117, val_loss: 0.148, accuracty: 0.957\n",
      "epoch: 19, iteration: 1500, train_loss: 0.119, val_loss: 0.146, accuracty: 0.958\n",
      "epoch: 19, iteration: 2000, train_loss: 0.12, val_loss: 0.146, accuracty: 0.959\n",
      "epoch: 20, iteration: 500, train_loss: 0.104, val_loss: 0.144, accuracty: 0.958\n",
      "epoch: 20, iteration: 1000, train_loss: 0.108, val_loss: 0.142, accuracty: 0.959\n",
      "epoch: 20, iteration: 1500, train_loss: 0.119, val_loss: 0.142, accuracty: 0.959\n",
      "epoch: 20, iteration: 2000, train_loss: 0.108, val_loss: 0.14, accuracty: 0.96\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ScratchSimpleNeuralNetrowkClassifier()\n",
    "model.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 8.\n",
    "h, w = 6, 6\n",
    "num = h * w\n",
    "y_pred = model.predict(x_test)\n",
    "falses = (y_pred != y_test).nonzero()[0][:num]\n",
    "num = len(falses)\n",
    "fig = plt.figure(figsize=(h, w))\n",
    "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "for i in range(num):\n",
    "    ax = fig.add_subplot(h, w, i + 1, xticks=[], yticks=[])\n",
    "    ax.set_title(\"{} / {}\".format(y_pred[falses[i]], y_test[falses[i]]))\n",
    "    ax.imshow(x_test[falses[i]].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前期の増田さんのコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチ処理のクラス\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=None):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクラッチ　ニューラルネットワーク\n",
    "class ScratchSimpleNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, n_epoch=50, n_features=784, n_nodes1=400, n_nodes2=200, \n",
    "                 n_output=10, sigma=0.01, n_batch=20, \n",
    "                 activate_function_key='tanh', lr = 0.01, verbose = False):\n",
    "        \n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.sigma = sigma\n",
    "        self.n_batch = n_batch\n",
    "        self.activate_function_key = activate_function_key\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def initial_weight(self):\n",
    "        self.W1 = self.sigma * np.random.randn(self.n_features, self.n_nodes1)\n",
    "        self.b1 = np.zeros(self.n_nodes1)\n",
    "        self.W2 = self.sigma * np.random.randn(self.n_nodes1, self.n_nodes2)\n",
    "        self.b2 = np.zeros(self.n_nodes2)\n",
    "        self.W3 = self.sigma * np.random.randn(self.n_nodes2, self.n_output)\n",
    "        self.b3 = np.zeros(self.n_output)\n",
    "        \n",
    "    def activation_function(self,X):\n",
    "        if self.activate_function_key == 'sigmoid':\n",
    "            return 1/(1+np.exp(-X))\n",
    "        \n",
    "        elif self.activate_function_key == 'tanh':\n",
    "            return np.tanh(X)\n",
    "    \n",
    "    def softmax(self,X):\n",
    "        \n",
    "        return np.exp(X-np.max(X))/np.sum(np.exp(X-np.max(X)),axis=1,keepdims=True)\n",
    "    \n",
    "    def loss_function(self,y,yt):\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "    \n",
    "    def gradient_descent(self,X,y,yt):\n",
    "        \n",
    "            # 3層目\n",
    "            delta_a3 = (y-yt)/self.n_batch\n",
    "            delta_b3 = np.sum(delta_a3,axis=0)\n",
    "            delta_W3 = np.dot(self.z2.T,delta_a3)\n",
    "            delta_z2 = np.dot(delta_a3,self.W3.T)\n",
    "        \n",
    "            self.W3 -= self.lr*delta_W3\n",
    "            self.b3 -= self.lr*delta_b3\n",
    "        \n",
    "            # 2層目\n",
    "            if self.activate_function_key == 'sigmoid':\n",
    "                delta_a2 = delta_z2*(1-self.activation_function(self.z2))*self.activation_function(self.z2)\n",
    "            \n",
    "            elif self.activate_function_key == 'tanh':\n",
    "                delta_a2 = delta_z2*(1-np.tanh(self.z2)**2)\n",
    "            \n",
    "            delta_b2 = np.sum(delta_a2,axis=0)\n",
    "            delta_W2 = np.dot(self.z1.T,delta_a2)\n",
    "            delta_z1 = np.dot(delta_a2,self.W2.T)\n",
    "        \n",
    "            self.W2 -= self.lr*delta_W2\n",
    "            self.b2 -= self.lr*delta_b2\n",
    "        \n",
    "            # 1層目\n",
    "            if self.activate_function_key == 'sigmoid':\n",
    "                delta_a1 = delta_z1*(1-self.activation_function(self.z1))*self.activation_function(self.z1)\n",
    "            \n",
    "            elif self.activate_function_key == 'tanh':\n",
    "                delta_a1 = delta_z1*(1-np.tanh(self.z1)**2)\n",
    "                \n",
    "            delta_b1 = np.sum(delta_a1,axis=0)\n",
    "            delta_W1 = np.dot(X.T,delta_a1)\n",
    "        \n",
    "            self.W1 -= self.lr*delta_W1\n",
    "            self.b1 -= self.lr*delta_b1\n",
    "                \n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        # 重みの初期化\n",
    "        self.initial_weight()\n",
    "        \n",
    "        # エポックごとのloss_functionを記録するリスト\n",
    "        self.log_loss = []\n",
    "        self.log_loss_val = []\n",
    "        \n",
    "        # エポックごとのTrainデータの推定を評価：Accuracy\n",
    "        self.log_acc = []\n",
    "        self.log_acc_val = []\n",
    "        \n",
    "        for epoch in range(self.n_epoch):\n",
    "            # ミニバッチ処理\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            \n",
    "            self.loss = 0\n",
    "            self.true_y = np.array([])\n",
    "            self.pred_y = np.array([])\n",
    "            \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "            \n",
    "                # 1層目\n",
    "                self.z1 = self.activation_function(np.dot(mini_X_train,self.W1) + self.b1)\n",
    "            \n",
    "                # 2層目\n",
    "                self.z2 = self.activation_function(np.dot(self.z1,self.W2) + self.b2)\n",
    "            \n",
    "                # 3層目（ソフトマックス関数）\n",
    "                yhat = self.softmax(np.dot(self.z2,self.W3) + self.b3)\n",
    "                \n",
    "                # バックプロパゲーション（確率的勾配降下法）\n",
    "                self.gradient_descent(mini_X_train,yhat,mini_y_train)\n",
    "                \n",
    "                # ミニバッチデータの正解値、推定値を記録\n",
    "                self.true_y = np.concatenate([self.true_y,np.argmax(mini_y_train,axis=1)])\n",
    "                self.pred_y = np.concatenate([self.pred_y,np.argmax(yhat,axis=1)])\n",
    "                \n",
    "                # 損失関数\n",
    "                self.loss += self.loss_function(yhat,mini_y_train)\n",
    "            \n",
    "            # epochごとに損失関数を記録\n",
    "            self.log_loss.append(self.loss/len(get_mini_batch))\n",
    "            \n",
    "            # Accuracy\n",
    "            acc = accuracy_score(self.true_y, self.pred_y)\n",
    "            self.log_acc.append(acc)\n",
    "            \n",
    "            # Valデータが入力されたら計算\n",
    "            if (type(X_val) != bool):\n",
    "                # 1層目\n",
    "                self.z1_val = self.activation_function(np.dot(X_val,self.W1) + self.b1)\n",
    "            \n",
    "                # 2層目\n",
    "                self.z2_val = self.activation_function(np.dot(self.z1_val,self.W2) + self.b2)\n",
    "            \n",
    "                # 3層目（ソフトマックス関数）\n",
    "                yhat_val = self.softmax(np.dot(self.z2_val,self.W3) + self.b3)\n",
    "                \n",
    "                # 損失関数\n",
    "                self.loss_val = self.loss_function(yhat_val,y_val)\n",
    "                self.log_loss_val.append(self.loss_val)\n",
    "                \n",
    "                # Accuracy\n",
    "                \n",
    "                acc_val = accuracy_score(np.argmax(y_val,axis=1), np.argmax(yhat_val,axis=1))\n",
    "                self.log_acc_val.append(acc_val)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                print('epoch:{:>3} loss:{:>8,.3f} acc:{:>5,.3f}'.format(epoch,self.loss/self.n_batch,acc))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        # 1層目\n",
    "        self.pred_z1 = self.activation_function(np.dot(X,self.W1) + self.b1)\n",
    "            \n",
    "        # 2層目\n",
    "        self.pred_z2 = self.activation_function(np.dot(self.pred_z1,self.W2) + self.b2)\n",
    "        \n",
    "        return np.argmax(np.dot(self.pred_z2,self.W3) + self.b3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train,x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = (y_train.reshape(-1, 1) == np.arange(10)).astype(np.float64)\n",
    "y_val_one_hot = (y_val.reshape(-1, 1) == np.arange(10)).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss:   1.101 acc:0.267\n",
      "epoch:  1 loss:   1.068 acc:0.372\n",
      "epoch:  2 loss:   0.802 acc:0.434\n",
      "epoch:  3 loss:   0.502 acc:0.667\n",
      "epoch:  4 loss:   0.361 acc:0.780\n",
      "epoch:  5 loss:   0.301 acc:0.816\n",
      "epoch:  6 loss:   0.265 acc:0.840\n",
      "epoch:  7 loss:   0.238 acc:0.857\n",
      "epoch:  8 loss:   0.218 acc:0.869\n",
      "epoch:  9 loss:   0.205 acc:0.877\n",
      "epoch: 10 loss:   0.195 acc:0.884\n",
      "epoch: 11 loss:   0.187 acc:0.889\n",
      "epoch: 12 loss:   0.180 acc:0.893\n",
      "epoch: 13 loss:   0.175 acc:0.897\n",
      "epoch: 14 loss:   0.170 acc:0.899\n",
      "epoch: 15 loss:   0.166 acc:0.902\n",
      "epoch: 16 loss:   0.162 acc:0.904\n",
      "epoch: 17 loss:   0.159 acc:0.906\n",
      "epoch: 18 loss:   0.155 acc:0.908\n",
      "epoch: 19 loss:   0.153 acc:0.910\n",
      "epoch: 20 loss:   0.150 acc:0.911\n",
      "epoch: 21 loss:   0.147 acc:0.913\n",
      "epoch: 22 loss:   0.145 acc:0.914\n",
      "epoch: 23 loss:   0.143 acc:0.915\n",
      "epoch: 24 loss:   0.140 acc:0.916\n",
      "epoch: 25 loss:   0.138 acc:0.918\n",
      "epoch: 26 loss:   0.136 acc:0.918\n",
      "epoch: 27 loss:   0.134 acc:0.920\n",
      "epoch: 28 loss:   0.133 acc:0.921\n",
      "epoch: 29 loss:   0.131 acc:0.922\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3c7404dbf4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "clf = ScratchSimpleNeuralNetrowkClassifier(n_epoch=30, n_features=784,\n",
    "                                           n_nodes1=400, n_nodes2=200, n_output=10,\n",
    "                                           sigma=0.01, n_batch=100,\n",
    "                                           activate_function_key='tanh',\n",
    "                                           lr = 0.01, verbose = True)\n",
    "\n",
    "clf.fit(x_train,y_train_one_hot,x_val,y_val_one_hot)\n",
    "y_pred = clf.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochごとの損失関数の可視化\n",
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "plt.plot(clf.log_loss,'rs--')\n",
    "plt.plot(clf.log_loss_val,'bo--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochごとの正解率の可視化\n",
    "fig = plt.subplots(figsize=(12,8))\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "plt.plot(clf.log_acc,'rs--')\n",
    "plt.plot(clf.log_acc_val,'bo--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5 # いくつ表示するか\n",
    "print('推定結果/正解')\n",
    "\n",
    "true_false = y_pred==y_val\n",
    "false_list = np.where(true_false==False)[0].astype(np.int)\n",
    "if false_list.shape[0] < num:\n",
    "    num = false_list.shape[0]\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "for i in range(num):\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
    "    ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今期の佐藤さんのコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
